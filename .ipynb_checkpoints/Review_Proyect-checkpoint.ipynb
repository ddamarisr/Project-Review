{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MOBHjoflYz-W"
   },
   "source": [
    "# **Proyecto predicción basada en Lenguaje Natural**\n",
    "---\n",
    "### *Introducción a la Inteligencia Artificial 2019-2*\n",
    "### *Diciembre de 2019*\n",
    "## Integrantes\n",
    "\n",
    "* Daniel Amaris(ddamarisr@unal.edu.co) - Ciencias de la Computacion\n",
    "* Christian Ortiz(cfortizp@unal.edu.co) - Ingeniería de Sistemas\n",
    "* Tania Vasquez(tvvasquezg@unal.edu.co) - Ciencias de la Computacion\n",
    "\n",
    "**Nota:** Recomendamos no correr todas las celdas, ya que hay procesos que toman bastante tiempo, en el informe se detallan cuales son. Tambien recomendamos el uso de Jupyter.\n",
    "\n",
    "# **1. Introducción**\n",
    "\n",
    "## **Definición del problema**\n",
    "\n",
    "El NLP (Natural Language Processing) es un paradigma muy importante para las Ciencias de la Computación en general y para el Machine Learning en específico. En un problema de NLP buscamos que los computadores sean capaces de comunicarse con las personas en el lenguaje cotidiano de estas últimas, bien sea para recibir instrucciones, información o primordialmente, para interactuar. \n",
    "\n",
    "En este problema en específico, se presentaron dos retos principales, siendo el primero de ellos el problema de representación y el segundo el problema de clasificación.\n",
    "\n",
    "![Captura.PNG](https://i.imgur.com/b0fo9KF.png)\n",
    "\n",
    "* Problema de Representación: \n",
    "Un computador no entiende las palabras humanas, tenemos que diseñar una estrategia para convertir oraciones (en este caso críticas) en magnitudes sobre las que puedan realizarse todas las operaciones necesarias. \n",
    "\n",
    "* Problema de Clasificación:\n",
    "Dada una crítica ¿cómo puede hacer el computador para predecir una calificación en una escala de 1 a 5 basándose en lo que está escrito en ella? Este problema puede abordarse de muchas formas diferentes, gracias a todos los algoritmos disponibles para realizar estas tareas. No obstante, como veremos luego, ajustar un modelo a cada problema es una tarea ardua y aparecerán problemas derivados dependiendo de la aproximación que tomemos.\n",
    "\n",
    "## **Estado del arte**\n",
    "\n",
    "Con el procesamiento de lenguaje natural podemos realizar innumerables tareas, dentro de ellas se encuentran la de chatbots, traducción, respuesta de preguntas, modelamiento de lenguaje, análisis de sentimientos, resúmenes de textos, generación de datos, clasificación de texto entre muchas otras, pero siendo esta la clasificación de textos la nos compete en nuestro proyecto miremos un poco más a fondo, entonces ¿Que es la clasificación de textos? Es la tarea de asignar una oración o documento una categoría apropiada, estas categorías dependen del conjunto de datos elegido y pueden variar. En nuestro caso la opinión de un estudiante respecto a un profesor y la categoría siendo la calificación.\n",
    "\n",
    "Revisando los artículos recientes de AG News DBpedia y TREC encontramos varios modelos NLP de clasificación de textos los cuales son evaluados en función del menor error\n",
    "\n",
    "\n",
    "**XLNet: preentrenamiento generalizado autorregresivo para la comprensión del lenguaje**\n",
    "\n",
    "En el 2019 un grupo de investigadores de Google Brain y de la Universidad Carnegie Mellon desarrollaron XLNet, un modelo de IA capaz de superar el BERT en 20 tareas de NLP logrando muy buenos resultados. BERT ( Representaciones de codificador bidireccional de Transform ) es el modelo de representación de lenguaje de Google para la capacitación previa sin supervisión de modelos de NLP\n",
    "\n",
    "XLNet es un método de preentrenamiento autorregresivo generalizado que permite aprender el aprendizaje de contexto bidireccional maximizando la probabilidad esperada sobre todas las permutaciones del orden de factorización y supera las limitaciones de BERT gracias a su formulación autorregresiva \n",
    "\n",
    "Segun DBpedia se reliazo una evaluacion de XLNet con 560,000 muestras de entrenamiento y 70,000 muestras de prueba para cada una de las 14 clases no superpuestas dando como resultado 0.62 de error\n",
    "\n",
    "**BERT: Representaciones de codificador bidireccional de transformadores**\n",
    "BERT está diseñado para entrenar previamente representaciones bidireccionales profundas a partir de texto sin etiquetar, reliazando un condicionamiento conjuntamente el contexto izquierdo y derecho en todas las capas. Como resultado, el modelo BERT pre-entrenado se puede ajustar con solo una capa de salida adicional para crear modelos de última generación para una amplia gama de tareas, siendo mut util para  la respuesta a preguntas y la inferencia del lenguaje, sin una tarea sustancial.\n",
    "BERT Obtiene nuevos resultados de vanguardia en once tareas de procesamiento del lenguaje natural, incluido el aumento de la puntuación GLUE al 80.5% (mejora absoluta de 7.7%), la precisión MultiNLI al 86.7% (mejora absoluta del 4.6%), SQuAD v1.1 pregunta respondiendo Prueba F1 a 93.2 (1.5 puntos de mejora absoluta) y SQuAD v2.0 Prueba F1 a 83.1 (5.1 puntos de mejora absoluta).\n",
    "\n",
    "en la misma evaluacion de DBPedia BERT obtuvo 0.64 \n",
    "\t\n",
    " otros modelos que se encontraron fueron los de Codificador de oración universal, DRNN Redes neuronales recurrentes desconectadas para categorización de texto, Modelo de bolsa de entidades atento neuronal para la clasificación de texto, SGCN Simplificación de redes convolucionales gráficas, Modelado discriminatorio de oraciones neuronales por convolución basada en árboles, maquina de vectores de soporte entre otros.\n",
    "\n",
    "\n",
    "\n",
    "**SVM:Máquina de vectores de soporte**\n",
    "<br>\n",
    "Modelos como el de Máquina de vectores de soporte (SVM) tambien denominados métodos kernel o máquinas kernel. Son un modelo de aprendizaje supervizado el cual contiene algoritmos de clasificacion y regresion, asiganando un conjunto de ejemplos de una categoria.\n",
    "\n",
    "Una  construye un hiperplano o un conjunto de hiperplanos en un espacio de dimensión alta o infinita, que puede usarse para la clasificación , la regresión u otras tareas como la detección de valores atípicos. El hiperplano logra una buena separación (denominado margen funcional), ya que en general cuanto mayor es el margen, menor es el error de generalización de el clasificador.\n",
    "\n",
    "Este algoritmos consta de dos partes la primera es la de transformacion de los datos de entrada en un espacio de caracteristicas de verias dimensiones, es decir especificar el Kernel, mas conocido como el truco de Kernel. \n",
    "La segunda parte es la de resolver un problema de optimizacion cuadratica que se ajuste a un hiperplano con el fin de clasificar las caracteristicas, las caracteristicas dependen del numero de vectores de sorporte, la superficie de decisión solo requiere los vectores de soporte seleccionados de los datos de entrenamiento, los demas ya no son relevantes\n",
    "\n",
    "Kernels mas usados:\n",
    "\n",
    "*   Función de base radial (RBF) o gaussiana K(x1,x2)=exp(−∥x1−x2∥22σ2)\n",
    "*   Lineal\tK(x1,x2)=x1^T*x2 \n",
    "*   Polinómica\tK(x1,x2)=(x1^T*x2+1)^ρ\n",
    "*   Sigmoid K(x1,x2)=tanh(β0x1^T*x2+β1)\n",
    "\n",
    "En el 2016 Van-Tu y Anh-Cuong relizaron un articulo llamado Mejora de la clasificación de preguntas por extracción y selección de características empleando el SPV teniendo un error de 8.4 en 50 clases y resultados  de precisiones de 95.2% y 91.6% para los conjuntos de datos de grano grueso y grano fino respectivamente, que son mucho mejores en comparación con los estudios previos.\n",
    "\n",
    "![texto alternativo](https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/300px-SVM_margin.png)\n",
    "\n",
    "**Random Forest: Bosque aleatorio**\n",
    "<br>\n",
    "Es un algorirmo clasificador por conjuntos que realiza estimaciones de acuerdo a la combinacion de diversos arboles de decision este se ajusta a varios clasificadores de árbol de decisión en varias submuestras del conjunto de datos, cada árbol del bosque se basa en un mejor subconjunto aleatorio de características, dichos árboles nos dan el mejor subconjunto de características entre todos los subconjuntos aleatorios de características.\n",
    "\n",
    "Funciona en cuatro pasos:\n",
    "\n",
    "Primero se seleccionan muestras aleatorias de un conjunto de datos dado.\n",
    "\n",
    "Segundo se construye un árbol de decisión para cada muestra y obtiene un resultado de predicción de cada árbol de decisión.\n",
    "\n",
    "Tercero se realiza una votación para cada resultado previsto.\n",
    "\n",
    "Cuarto Se selecciona el resultado de la predicción con más votos como la predicción final.\n",
    "\n",
    "![texto alternativo](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1526467744/voting_dnjweq.jpg)\n",
    "\n",
    "**Ventajas**\n",
    "\n",
    "Son considerados un método altamente preciso y robusto debido a la cantidad de árboles de decisión que participan en dicho proceso.\n",
    "Como toman el promedio de todas las predicciones no teinen el problema  de sobreajuste,ya que esto cancela los sesgos.\n",
    "Este puede en problemas de clasificación como de regresión.\n",
    "Pueden manejar valores perdidos de dos formas, usar valores medios para reemplazar variables continuas y calcular el promedio ponderado de proximidad de los valores faltantes o obtener la importancia relativa de la característica, que ayuda a seleccionar las características más contribuyentes para el clasificador.\n",
    "\n",
    "**Desventajas**\n",
    "Son lentos en la generación de predicciones debido a los múltiples árboles de decisión. \n",
    "\n",
    "En Modelado discriminatorio de oraciones neuronales por convolución basada en árboles con 6 clases se encontro un error de 4 esto en el año 2015.\n",
    "\n",
    "## **Librerías**\n",
    "\n",
    "* **csv:** El módulo csv implementa clases para leer y escribir datos tabulares en formato CSV. Permite a los programadores decir, \"escribir estos datos en el formato preferido por Excel\", o \"leer los datos de este archivo generado por Excel\", sin conocer los detalles precisos del formato CSV utilizado por Excel. Los programadores también pueden describir los formatos CSV que entienden otras aplicaciones o definir sus propios formatos CSV especiales.\n",
    "* **re:** Este módulo proporciona operaciones de correspondencia de expresiones regulares similares a las que se encuentran en Perl.\n",
    "* **nltk:** Natural Language Processing es un modulo que proporciona herramientas para el procesamiento del lenguaje.\n",
    "* **math:** Este módulo proporciona acceso a las funciones matemáticas definidas por el estándar C.\n",
    "* **numpy:** NumPy es el paquete fundamental para la computación científica con Python. Contiene, entre otras cosas, un poderoso objeto de matriz N-dimensional, álgebra lineal útil, transformada de Fourier y capacidad de números aleatorios.\n",
    "* **imblearn:** Es un paquete de python que ofrece una serie de técnicas de remuestreo comúnmente utilizadas en conjuntos de datos que muestran un fuerte desequilibrio entre clases. Es compatible con scikit-learn y forma parte de proyectos scikit-learn-contrib.\n",
    "* **sklearn:** Es una biblioteca en Python que proporciona muchos algoritmos de aprendizaje no supervisados y supervisados. Se basa en alguna de las tecnologías con las que ya está familiarizado, como NumPy, pandas y Matplotlib.\n",
    "\n",
    "La siguiente celda es importante que la corra una solá vez. En está se instalan librerias que no hacen parte del entorno de Jupyter. Para la instalación de nltk seleccione all corpora como muestra la imagen.\n",
    "\n",
    "![Imgur](https://i.imgur.com/1NuKDGH.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7b3-TgeRkVUT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk in c:\\users\\cfort\\anaconda3\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied, skipping upgrade: six in c:\\users\\cfort\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n",
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\cfort\\anaconda3\\lib\\site-packages (0.5.0)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\users\\cfort\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.16.5)\n",
      "Requirement already satisfied: scipy>=0.17 in c:\\users\\cfort\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.3.1)\n",
      "Requirement already satisfied: scikit-learn>=0.21 in c:\\users\\cfort\\anaconda3\\lib\\site-packages (from imbalanced-learn) (0.21.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\cfort\\anaconda3\\lib\\site-packages (from imbalanced-learn) (0.13.2)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -U nltk\n",
    "import nltk\n",
    "nltk.download()\n",
    "!{sys.executable} -m pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pkbCxD-EoUZP"
   },
   "source": [
    "Se importan las librerías necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_HQalHZLi3zV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cfort\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn import datasets \n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "crHWXxtbi90G"
   },
   "source": [
    "# **2. Recolección de datos y creación de la base de datos**\n",
    "\n",
    "## **Obtención**\n",
    "\n",
    "Con ayuda de la extensión para Google Chrome webscraper.io, disponible aquí: https://webscraper.io/ construimos un script para realizar webscrapping a la página https://losestudiantes.co/. \n",
    "\n",
    "![Imgur](https://i.imgur.com/jxUrDD3.png)\n",
    "\n",
    "El Scraper uso el siguiente JSON:\n",
    "\n",
    "![Imgur](https://i.imgur.com/heWZx5L.png)\n",
    "\n",
    "El Scrapping rescató dos elementos principales por cada profesor de cada universidad (Universidad de los Andes y Universidad Nacional): Todas sus críticas y la nota correspondiente a cada una de ellas y se almacenaron en archivos **csv**. Todos los archivos csv se encuentran en la carpeta data, que tendra a su disposición. Una vez se tienen los archivos se procesan los archivos en la función **reviewProcessing()**. Al final se obtuvieron 21984 críticas válidas con sus calificaciones correspondientes y se consignaron en los archivos 'datafile.txt' y 'original_scores.txt' respectivamente.\n",
    "\n",
    "Es necesario, realizar un proceso de normalización de los datos obtenidos, para ser precisos de los comentarios. Para esto dado que es problema basado en lenguaje natural, se vuelve imprescindible modificar la estructura de los comentarios. Esto se hace a través de la funcion **normalizeReview()**, que se encuentra en la siguiente celda. En esta función se realizan los siguientes procedimientos:\n",
    "* Se convierte el texto a minúscula.\n",
    "* Se remplazan palabras y expresiones como \"pros\", \"cons\" y \"leer más...\" por espacios. Ya que estas son palabras de la pagina que no tienen ninguna importancia dentro del análisis y son muy frecuentes, lo cual sesgaría el modelo.\n",
    "* Se remplazán numeros y expresiones numéricas por espacios, ya que estas no tiene ninguna importancia dentro de la clasificación.\n",
    "* Se eliminan **stopwords** que se encuentran en el comentario. Las stopwords hace referencia a palabras sin significado como artículos, pronombres, preposiciones, entre otros. Ya que estas no ayuda a la clasificación.\n",
    "* Se eliminan signos de puntuación los cuales son frencuentes y pueden sergar el modelo.\n",
    "* Se eliminan los afijos morfológicos de las palabras, dejando sólo la raiz de las palabras.\n",
    "* Finalmente, se eliminan los saltos de linea y se escriben las palabras resultantes en una linea separadas por espacios.\n",
    "\n",
    "Cabe resaltar que la mayoria de estos procedimientos se realizan a través de la libreria nltk de Python. A continuación puede ver un ejemplo de un comentario que ha sido procesado a través de esta función.\n",
    "\n",
    "**Comentario sin Procesar**\n",
    "\n",
    "![texto alternativo](https://i.imgur.com/8zPyam3.png)\n",
    "\n",
    "**Comentario Procesado**\n",
    "\n",
    "![texto alternativo](https://i.imgur.com/6xVY0Mf.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O4x3kesX6FOY"
   },
   "outputs": [],
   "source": [
    "def normalizeReview(review):\n",
    "    \n",
    "    temp=review.lower(); #pasa la cadena a minusculas\n",
    "    temp=temp.replace(\"pros\",\" \")\n",
    "    temp=temp.replace(\"cons\",\" \")\n",
    "    temp=temp.replace(\"leer más...\",\" \")\n",
    "    temp = re.sub(r\"[+-]?([0-9]*[.])?[0-9]+\",\" \",temp) #reemplaza los numeros por espacios sencillos\n",
    "   \n",
    "    tokens = [word for word in wordpunct_tokenize(temp)] #separar cada palabra\n",
    "    stopw = [w for w in stopwords.words('spanish')] #stopw son palabras que seran ignoradas\n",
    "                                                                    \n",
    "    punc_lit = ['.', '[', ']', ',', ';', '', ')', '),', ' ', '(',\n",
    "    '!','¡','-','¿','?',':',\"pros\",\"cons\",'\"']\n",
    "    \n",
    "    stopw.extend(punc_lit) #agregar la puntuacion a la lista de stopwords\n",
    "    \n",
    "    words = [token for token in tokens if token not in stopw] #recupera los tokens que no estan en stopwords\n",
    "    Snowball_stemmer = SnowballStemmer('spanish')\n",
    "    stemmers2 = [Snowball_stemmer.stem(word) for word in words]\n",
    "    final2 = [stem for stem in stemmers2 if stem.isalpha() and len(stem) > 1]\n",
    "    \n",
    "    res = ' '.join([word for word in final2])\n",
    "    \n",
    "    \n",
    "    return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e2RwQDElo1bN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1437 lines.\n",
      "A1 1437\n",
      "Processed 1230 lines.\n",
      "A2 1230\n",
      "Processed 748 lines.\n",
      "A3 748\n",
      "Processed 270 lines.\n",
      "A4 270\n",
      "Processed 356 lines.\n",
      "A5 356\n",
      "Processed 370 lines.\n",
      "A6 370\n",
      "Processed 48 lines.\n",
      "A7 48\n",
      "Processed 1491 lines.\n",
      "A8 1491\n",
      "Processed 703 lines.\n",
      "A9 703\n",
      "Processed 1073 lines.\n",
      "A10 1073\n",
      "Processed 157 lines.\n",
      "A11 157\n"
     ]
    }
   ],
   "source": [
    "def reviewProcessing():\n",
    "    \n",
    "    trainingExamples=0\n",
    "    university=\"A\"\n",
    "    counter=1;\n",
    "    f = open('datafile.txt', 'w+')\n",
    "    f.close()\n",
    "    f = open('scores.txt', 'w+')\n",
    "    f.close()\n",
    "    f = open('Original_scores.txt', 'w+')\n",
    "    f.close()\n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        path=university+str(counter)\n",
    "        normalizedTexts=[]\n",
    "        scores=[]\n",
    "        \n",
    "        if(path==\"A33\"):\n",
    "            counter=1\n",
    "            university=\"N\"\n",
    "            \n",
    "        elif(path==\"N36\"):\n",
    "            break;\n",
    "            \n",
    "        else:   \n",
    "            with open ('data/'+path+'.csv',encoding='utf-8') as csv_file:\n",
    "                csv_reader=csv.reader(csv_file,delimiter=\",\")\n",
    "                line_count=0\n",
    "                for row in csv_reader:\n",
    "                    if line_count==0:\n",
    "                        line_count+=1\n",
    "                    elif(row[5]!='null'and row[4]!='null'):\n",
    "                        normalizedTexts.append(normalizeReview(row[4]))\n",
    "                        scores.append(row[5])\n",
    "                        line_count += 1\n",
    "                       \n",
    "                print(f'Processed {line_count-1} lines.')\n",
    "                \n",
    "                print(path,line_count-1)\n",
    "                \n",
    "                trainingExamples=trainingExamples+line_count-1\n",
    "            \n",
    "            with open('datafile.txt', 'a+') as f:\n",
    "                for item in normalizedTexts:\n",
    "                    f.write(\"%s\\n\" % item)\n",
    "            f.close()\n",
    "            \n",
    "            with open('scores.txt', 'a+') as f:\n",
    "                for item in scores:\n",
    "                    f.write(\"%s\\n\" % str(item))\n",
    "            f.close()\n",
    "            with open('Original_scores.txt', 'a+') as f:\n",
    "                for item in scores:\n",
    "                    f.write(\"%s\\n\" % str(item))\n",
    "            f.close()\n",
    "            \n",
    "            counter+=1\n",
    "    \n",
    "    print(trainingExamples,\"ejemplos totales\")\n",
    "\n",
    "reviewProcessing()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5B3rvjtciIy3"
   },
   "source": [
    "## **Frecuencia de los datos**\n",
    "\n",
    "Veamos la distribución de los datos tal y como fueron extraídos del sitio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mjcO_S_ZZl8m"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualizeHistogram1(y,title,n_bins):\n",
    "    \n",
    "    #y=y.astype('str')\n",
    "    y=np.sort(y)\n",
    "    \n",
    "    unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "    \n",
    "    \n",
    "    n, bins, patches = plt.hist(x=y, bins=range(n_bins), color='r',rwidth=0.85)\n",
    "\n",
    "    \n",
    "    plt.grid(axis='y')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.show()\n",
    "\n",
    "scores=[]\n",
    "\n",
    "f= open('Original_scores.txt', 'r')\n",
    "\n",
    "for item in f:\n",
    "    scores.append(str(item.split(\"\\n\")[0]))\n",
    "f.close()\n",
    "\n",
    "y=np.asarray(scores).astype(str)\n",
    "\n",
    "visualizeHistogram(y,\"Frecuencia dataset original\",43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZOlgoRSABYIn"
   },
   "source": [
    "# **3. Manejo y Representación de los datos**\n",
    "\n",
    "### **Creación del Diccionario**\n",
    "Una vez almacenados y normalizados los datos en el archivo datafile.txt, creamos una lista de las 1500 palabras más comunes del dataset. Para esto utilizamos la función **wordFreq()** y **vocabularyList()**, estas palabras se almacenan en el archivo dictionary.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sjDeCkby3PRT"
   },
   "outputs": [],
   "source": [
    "def wordFreq():\n",
    "    \n",
    "    wordFreq={} #diccionario\n",
    "    \n",
    "    f = open(\"datafile.txt\", \"r\")\n",
    "    \n",
    "    for line in f:\n",
    "        splitLine=line.split()\n",
    "        for word in splitLine:\n",
    "            if(word in wordFreq):# Si ya está solo diga que la palabra aparecio una\n",
    "                wordFreq[word] += 1 # vez más\n",
    "            else:\n",
    "                wordFreq[word]= 1 # agregue la entrada\n",
    "    f.close()            \n",
    "    \n",
    "    return wordFreq \n",
    "\n",
    "def vocabularyList():\n",
    "    k=1500 # el número de palabras que constituyen el diccionario\n",
    "    vocabularyList=[]\n",
    "    countedWords=wordFreq()\n",
    "    \n",
    "    for words in countedWords.keys():\n",
    "        vocabularyList.append((words,countedWords[words]))\n",
    "    \n",
    "    vocabularyList=sorted(vocabularyList,key=lambda x: x[1], reverse=True) #Ordena esta lista de tuplas\n",
    "                                                                        #En orden descendente según la segunda componente\n",
    "    vocabularyList=vocabularyList[0:k] #solo las k mas frecuentes\n",
    "    \n",
    "    with open('dictionary.txt', 'w+') as f:\n",
    "        for item in vocabularyList:\n",
    "            f.write(\"%s\\n\" % item[0])\n",
    "    f.close()\n",
    "\n",
    "vocabularyList()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AC32A87X8FzQ"
   },
   "source": [
    "### **Representación Vectorial**\n",
    "\n",
    "Con el fin de respresentar los comentarios de manera unificada para los modelos clasificadores, cada uno de estos se represento como un vector de dimensiones (1,1500) donde se representá la frecuencia de las palabras del diccionario en el comentario. Cada uno de estos comentarios es almacenado en una matriz que tendrá dimensiones de (21984,1500), este proceso se hace en la función **dictionary_vector()**. Finalmente se normaliza la matriz gracias a la función normalize de scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8_IRplAC7VW5"
   },
   "outputs": [],
   "source": [
    "def dictionary_vector():\n",
    "    \n",
    "    words_dictionary = []\n",
    "    f = open(\"dictionary.txt\", \"r\")\n",
    "    for line in f:\n",
    "        words_dictionary.append(line.split(\"\\n\")[0])\n",
    "    f.close()\n",
    "    training_matrix = np.array([])\n",
    "    f = open(\"datafile.txt\", \"r\")\n",
    "    line_count = 0\n",
    "    listma = []\n",
    "    for line in f:\n",
    "        splitLine=line.split()\n",
    "        zeros = np.zeros(len(words_dictionary))\n",
    "        for word in splitLine:\n",
    "            if word in words_dictionary:\n",
    "                index = words_dictionary.index(word) \n",
    "                zeros[index]+=1\n",
    "        listma.append(zeros)\n",
    "        line_count+=1\n",
    "    f.close()\n",
    "    training_matrix = np.asarray(listma) \n",
    "    return training_matrix\n",
    "\n",
    "tm = dictionary_vector()\n",
    "tm = normalize(tm,axis=0)\n",
    "print(\"\\nHa cargado la matriz de datos con dimensiones de: \",np.shape(tm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HJlmik-QARKI"
   },
   "source": [
    "### **Balanceo de Datos**\n",
    "\n",
    "Dada la naturaleza de los scores de los comentario, era necesario hacer un ajuste de estos, pues como se pudo ver en un histograma anterior estaban totalmente desbalanceados las clases. Para poder hacer este proyecto un poco más realista (con los datos y los recursos disponibles es bastante irreal pensar en un problema de clasificación de 40 clases). Acotamos el problema a predecir cinco clases (enteros del 1 a 5) y aproximamos todas las calificaciones a su parte entera como se muestra en el siguiente histograma.. Todo esto se realizo en la función **balanceddata()**, los scores resultantes se escribieron en el archivo scores.txt. El resultado de está operación se puede evidenciar facilmente en el histograma generado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ck14VdyF-AI"
   },
   "outputs": [],
   "source": [
    "def balanceddata():\n",
    "    scores = []\n",
    "    f= open('scores.txt', 'r')\n",
    "    for item in f:\n",
    "        scores.append(str(math.floor(float((item.split(\"\\n\")[0])))))\n",
    "    f.close()\n",
    "    with open('scores.txt', 'w+') as f:\n",
    "        for item in scores:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    f.close()\n",
    "\n",
    "def visualizeHistogram(y,title):\n",
    "    \n",
    "    y=y.astype(int)\n",
    "    y=np.sort(y)\n",
    "   \n",
    "    n, bins, patches = plt.hist(x=y, bins=[1,2,3,4,5,6], color='r',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "    \n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "balanceddata()\n",
    "\n",
    "f= open('scores.txt', 'r')\n",
    "for item in f:\n",
    "    scores.append(str(item.split(\"\\n\")[0]))\n",
    "f.close()\n",
    "\n",
    "y=np.asarray(scores).astype(str)\n",
    "\n",
    "visualizeHistogram(y,\"Dataset ajustado con la parte entera\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ij3pciQMIYYS"
   },
   "source": [
    "Como se puede ver a pesar de que se redujo la diferencias entre las clases los datos siguen estando imbalanceados. Para eso dentro de los modelos se uso la función **SMOTE()** de imblearn. La cual aumenta el número de casos no representados en un conjunto de datos utilizado para el aprendizaje automático. Lo cual es la mejor manera de aumentar el número de casos raros que simplemente duplicar los casos existentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wj4ecIP54mbI"
   },
   "source": [
    "### **Partición de los datos**\n",
    "\n",
    "Para la partición de los datos se divieron los datos en 70% entrenamiento y 30% prueba. Y a su vez los de entrenamiento se particionarón a través de la validación cruzada de K pliegues. En validación cruzada, los datos son particionados varias veces en entrenamiento y validación de forma repetida. Finalmente el desempeño del clasificador es agregado sobre las diferentes particiones de validación para obtener un estimador más robusto.\n",
    "\n",
    "La validación cruzada se hace comúnmente de la siguiente manera:\n",
    "* Se divide el conjunto de entrenamiento en k-pliegues o particiones (usualmente 3, 5 o 10).\n",
    "* Estas particiones deben ser del mismo tamaño\n",
    "* En cada iteración uno de los pliegues es usado como la partición de validación, mientras el resto es usado como la partición de entrenamiento.\n",
    "* Se reporta y guarda el desempeño sobre esa partición de validación\n",
    "\n",
    "![texto alternativo](https://drive.google.com/uc?export=view&id=12nCvyPDNOgHuxpToJrlR-m71nt3SOrKo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "snA3bFbRTS2m"
   },
   "source": [
    "# **5. Metodología**\n",
    "Para solucionar este problema utilizamos dos algoritmos muy conocidos en el aprendizaje automatico y útilizados para el análisis de clasificación y regresión. Estos son Support Vector Machine(SVM), que es un modelo de aprendizaje no supervisado y Random Forest que es un método de aprendizaje conjunto basado en arboles.\n",
    "\n",
    "Antes de abor es necesario saber que el algoritmo de Random Forest es un modelo multiclase a diferencia de las SVM que para solucionar este tipo de problemas necesita soluciones alternativas.\n",
    "\n",
    "## **Support Vector Machine(SVM)** \n",
    "\n",
    "Las máquinas de vectores de soporte son un modelo de aprendizaje supervisado en el cual los ejemplos son representados en un nuevo espacio, de tal forma que aquellos ejemplos de diferentes categorías sea posible, en principio, separarlos linealmente. El objetivo del modelo es encontrar un hiperplano en un espacio N-dimensional (N - el número de características) que clasifica claramente los puntos de datos. \n",
    "\n",
    "![texto alternativo](https://drive.google.com/uc?export=view&id=1NuYvzWfEb_nOZ9pWbsycdQWiHhad5_BI)\n",
    "\n",
    "Para separar las dos clases de puntos de datos, hay muchos hiperplanos posibles que podrían elegirse. El objetivo es encontrar un plano que tenga el margen máximo, es decir, la distancia máxima entre los puntos de datos de las clases. Maximizar la distancia de margen proporciona cierto refuerzo para que los puntos de datos futuros se puedan clasificar con más confianza.\n",
    "\n",
    "### **Parametros de Ajuste** \n",
    "**Kernel**\n",
    "\n",
    "El aprendizaje del hiperplano en SVM lineal se realiza transformando el problema utilizando algo de álgebra lineal. Para esto SVM usa una función conocida como kernel, **k**, la cual define qué tan parecidas son dos instancias del conjunto de datos.\n",
    "\n",
    "La función  k  calcula el producto punto en el espacio de caracterísiticas donde se representarán los datos. Dependiendo del tipo de kernel, este espacio de características es de mayor dimensionalidad, y facilita la definición de un \"hiperplano\" que separe los ejemplos de ambas características.\n",
    "\n",
    "Existen varias opciones para las funciones de kernel, de las cuales solo usaremos dos, las cuales son:\n",
    "* Kernel Lineal (lineal)\n",
    "\n",
    "  Donde la función **k** está definida como:\n",
    "\n",
    "$$\n",
    "k(x,y) = \\langle x, y\\rangle = xy'\n",
    "$$\n",
    "* Kernel Gaussiano (RBF)\n",
    "\n",
    "  Donde la función **k** está definida como:\n",
    "  \n",
    "$$\n",
    "K(x, x') = \\exp(-\\gamma \\|x-x'\\|^2)\n",
    "$$\n",
    "$$\n",
    "\\gamma \\ = \\frac{1}{2\\sigma^2}\n",
    "$$\n",
    "\n",
    "**Regularización**\n",
    "\n",
    "En SVM, el parametro **C**, comocido como el parámetro Regularización, hace referencia a cuánto desea evitar clasificar erróneamente cada ejemplo de entrenamiento.\n",
    "\n",
    "Por ejemplo cuando **C** toma un valor muy pequeño, el optimizador buscará un hiperplano de separación de mayor margen, sin importar si si ese hiperplano clasifica erróneamente más puntos. Por otro lado si **C** toma un valor muy grande la optimización elegirá un hiperplano de menor margen si ese hiperplano hace un mejor trabajo al clasificar correctamente todos los puntos de entrenamiento.\n",
    "\n",
    "**Gamma**\n",
    "\n",
    "El parametro gamma, usado en el kernel gaussiano, en las SVM hace referencia a la influencia de un solo ejemplo de entrenamiento, cuando gamma tiene un valor bajo hace referencia a \"lejos\" y con valores altos \"cerca\".\n",
    "\n",
    "### **Implementación en Python** \n",
    "\n",
    "Para la implementación de las SVM importamos \"svm\" de la libreria scikit-learn.\n",
    "\n",
    "**Kernel Lineal**\n",
    "\n",
    "Recomendamos no correr la siguiente celda de codigo ya que en esta se utilza la función GridSearchCV que implementa los parámetros, en este caso el C de la SVM, que tienen un mejor rendimiento. Por esta razon el tiempo de ejecución puede tomar bastante tiempo. Más adelante se tiene un modelo entrenado teniendo en cuenta la salida de la función.\n",
    "\n",
    "![texto alternativo](https://i.imgur.com/Q5EFk2i.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OTtvWT4MgNY9"
   },
   "outputs": [],
   "source": [
    "def SVMtraininglineal(X,y):\n",
    "    if ( __name__ == \"__main__\"):\n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        from imblearn.pipeline import Pipeline\n",
    "        from sklearn.svm import SVC\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "        \t\t\n",
    "        sm = SMOTE()\n",
    "        sv = SVC(kernel='linear')\n",
    "        \t\t\n",
    "        grid = [i for i in range(-5,5, 1)]\n",
    "        \t\t\n",
    "        pipeline = Pipeline([('sm', sm), ('sv', sv)])\n",
    "        \t\t\n",
    "        \t\t\n",
    "        params = {'sv__C' :  [2**i for i in grid]}\n",
    "        \n",
    "        \n",
    "        grid = GridSearchCV(pipeline, param_grid=params,n_jobs=-1,cv=5,verbose=11,return_train_score=True)\n",
    "        \t\t\n",
    "        grid.fit(X_train, y_train)\n",
    "        \t\t\n",
    "        print(grid.best_params_)\n",
    "        \t\t\n",
    "        print(grid.best_score_)\n",
    "\n",
    "SVMtraininglineal(tm,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "awPlvlFegODi"
   },
   "source": [
    "**Kernel Gaussiano**\n",
    "\n",
    "Recomendamos no correr la siguiente celda de codigo ya que en esta se utilza la función GridSearchCV que implementa los parámetros, en este caso el C y el Gamma de la SVM, que tienen un mejor rendimiento. Por esta razon el tiempo de ejecución puede tomar bastante tiempo. Más adelante se tiene un modelo entrenado teniendo en cuenta la salida de la función.\n",
    "\n",
    "![Imgur](https://i.imgur.com/0HenAbr.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LltrTh1tuhqt"
   },
   "outputs": [],
   "source": [
    "def SVMtrainingrbf(X,y):\n",
    "    if ( __name__ == \"__main__\"):\n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        from imblearn.pipeline import Pipeline\n",
    "        from sklearn.svm import SVC\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "        \t\t\n",
    "        sm = SMOTE()\n",
    "        sv = SVC()\n",
    "        \t\t\n",
    "        grid = [i for i in range(-5,5, 1)]\n",
    "        \t\t\n",
    "        pipeline = Pipeline([('sm', sm), ('sv', sv)])\n",
    "        \t\t\n",
    "        \t\t\n",
    "        params = {'sv__C' :  [2**i for i in grid], 'sv__gamma' :  [2**i for i in grid]}\n",
    "        \n",
    "        \n",
    "        grid = GridSearchCV(pipeline, param_grid=params,n_jobs=-1,cv=5,verbose=11,return_train_score=True)\n",
    "        \t\t\n",
    "        grid.fit(X_train, y_train)\n",
    "        \t\t\n",
    "        print(grid.best_params_)\n",
    "        \t\t\n",
    "        print(grid.best_score_)\n",
    "\n",
    "SVMtraininrbf(tm,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d63DzLC6_G8O"
   },
   "source": [
    "## **Random Forest**\n",
    "\n",
    "Random Forest Classifier es un algoritmo de conjunto. Este metodo es una técnica de ensamble que combina diferentes árboles de decisión, los cuales se entrenan en diferentes muestras del conjunto de datos, estos árboles pueden sobreajustarse, sin embargo la combinación de sus predicciones resulta en un clasificador con menor sobreajuste.\n",
    "\n",
    "![texto alternativo](https://drive.google.com/uc?export=view&id=1_twQEtsjRjDHxcDh4cFGQ4cHnkKwmLY6)\n",
    "\n",
    "### **Parametros basicos**\n",
    "\n",
    "Los parámetros básicos para Random Forest Classifier pueden ser el número total de árboles que se generarán y los parámetros relacionados con el árbol de decisión, como la división mínima, los criterios de división, la profundidad entre otros.\n",
    "\n",
    "### **Algoritmo básico Random Forest**\n",
    "\n",
    "A diferencia del árbol común de decisión, los árboles de Random Forest se entrenan de una forma diferente. Como se puede ver a continuación el algoritmo:\n",
    "\n",
    "1. Para cada árbol se realiza el siguiente procedimiento:\n",
    "    * Se escoge una muestra con reemplazo de tamaño $n$ del conjunto de entrenamiento.\n",
    "    * Se seleccionan $m$ variables al azar de las variables disponibles\n",
    "    * Se entrena un árbol sobre la muestra usando las $m$ variables repitiendo los siguientes pasos:\n",
    "        * Se escoge la variable (y umbral) que genera la mejor partición\n",
    "        * Se divide los datos en dos subconjuntos de acuerdo a la variable y el umbral\n",
    "        * Si no se satisface un criterio de parada se aplica este procedimiento recursivamente sobre los subconjutos\n",
    "2. Una vez cada árbol ha sido entrenado, se genera el ensamble de árboles.\n",
    "\n",
    "### **Implementación en Python**\n",
    "\n",
    "Para la implementación de Random Forest se importa RandomForestClassifier de scikit-learn.\n",
    "\n",
    "Recomendamos no correr la siguiente celda de codigo ya que en esta se utilza la función RandomizedSearchCV que implementa una búsqueda aleatoria sobre los parámetros del Random Forest, que tienen un mejor rendimiento. Por esta razon el tiempo de ejecución puede tomar bastante tiempo. Más adelante se tiene un modelo entrenado teniendo en cuenta la salida de la función.\n",
    "\n",
    "![Imgur](https://i.imgur.com/YyODdH4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0d3geaKtxuol"
   },
   "outputs": [],
   "source": [
    "def randomForestSearch(X,y):\n",
    "    \n",
    "    if ( __name__ == \"__main__\"):\n",
    "        \n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "        #from sklearn.model_selection import train_test_split, GridSearchCV ,RandomizedSearchCV\n",
    "        \n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        \n",
    "        from imblearn.pipeline import Pipeline\n",
    "        \n",
    "        #from sklearn.svm import SVC\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "        \t\t\n",
    "        sm = SMOTE()\n",
    "        \n",
    "        rf = RandomForestClassifier()\n",
    "        \t\t\n",
    "        grid = [i for i in range(-3,4, 1)]\n",
    "        \t\t\n",
    "        pipeline = Pipeline([('sm', sm), ('sv', sv)]) \n",
    "        # Numeros arboles bosque\n",
    "        n_estimators = [int(x) for x in np.linspace(start = 200, stop = 4000, num = 15)] \n",
    "        # Cantidad de features que se van a evaluar\n",
    "        max_features = ['auto', 'sqrt']\n",
    "        # Profundad máxima  del árbol\n",
    "        max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "        max_depth.append(None)\n",
    "        # Minimo múmero de muestras requeridas para dividir nodo\n",
    "        min_samples_split = [2, 5, 10]\n",
    "        # Minimo numero de muestras en cada vértice\n",
    "        min_samples_leaf = [1, 2, 4]\n",
    "        # Método para tomar muestras\n",
    "        bootstrap = [True, False]\n",
    "\t\t\n",
    "        random_grid = {'rf__n_estimators': n_estimators,\n",
    "                       'rf__max_features': max_features,\n",
    "                       'rf__max_depth': max_depth,\n",
    "                       'rf__min_samples_split': min_samples_split,\n",
    "                       'rf__min_samples_leaf': min_samples_leaf,\n",
    "                       'rf__bootstrap': bootstrap\n",
    "                       }\n",
    "    \n",
    "        \n",
    "        from sklearn.model_selection import RandomizedSearchCV\n",
    "        \n",
    "        #Probar 30 combinaciones de parámtros\n",
    "        \n",
    "        grid = RandomizedSearchCV(pipeline, param_distributions=random_grid,n_jobs=-1,cv=3,n_iter=30,verbose=11,return_train_score=True)\n",
    "        \t\t\n",
    "        grid.fit(X_train, y_train)\n",
    "        \t\t\n",
    "        print(grid.best_params_) #Imprimir los mejores hiperparámetros\n",
    "        \t\t\n",
    "        print(grid.best_score_) # Imprimir el mejor accuracy\n",
    "\t\t\n",
    "randomForestSearch(tm,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9xqs0LZva747"
   },
   "source": [
    "# **5. Resultados**\n",
    "\n",
    "## **Métricas de desempeño**\n",
    "### **Matriz de Confusión** \n",
    "\n",
    "Está nos permite calcular otra serie de medidas para evaluar el desempeño del clasificador.\n",
    "\n",
    "![Imgur](https://drive.google.com/uc?export=view&id=1NwaZST2l_A8ohWZCdYt15zXq3rP0dbrw)\n",
    "\n",
    "\n",
    "Los componentes de esta matriz pueden interpretarse como:\n",
    "* TP: Verdaderos positivos.\n",
    "* TN: Verdaderos negativos.\n",
    "* FP: Falsos positivos.\n",
    "* FN: Falsos negativos.\n",
    "\n",
    "Tenga en cuentá que es un modelo multiclase, por lo tanto la matriz de confusión es de 5 x 5.\n",
    "\n",
    "### **Accuracy, precisión, recall y f1 score**\n",
    "\n",
    "De esta matriz se puede definir accuracy, precisión, recall y f1 score:\n",
    "\n",
    "* $accuracy = \\frac{TP + TN}{TP + FN + FP + TN}$\n",
    "* $PRE = \\frac{TP}{TP + FP}$ \n",
    "* $REC = \\frac{TP}{FN + TP}$ \n",
    "* $F_1 = 2 * \\frac{PRE*REC}{PRE + REC}$\n",
    "\n",
    "En un modelo multiclase se toma como positivo la clase que se está evaluando y como negativo el resto de las clases.\n",
    "\n",
    "La exactitud o accuracy mide cuantitativamente cuantas predicciones fueron correctas. La precisión se puede definir como la habilidad del clasificador de **no** clasificar una muestra como positiva cuando es negativa. Mientras el recall (índice de recuperación o sensitividad) se puede definir como la capacidad del clasificador de encontrar todas las muestras positivas.  $F_1 \\textit{score}$ se define como el promedio pesado de precisión y recall.\n",
    "\n",
    "## **Entrenamiento de los Modelos con Hiperparametros optimos**\n",
    "\n",
    "Para el entrenamiento de los modelos se tuvieron en cuenta los resultados del ajuste de los hiperparametros. En cada uno de estos modelos podra ver los valores de las metricas medidas y la matriz de confusión.\n",
    "\n",
    "No es necesario correr las celdas de entrenamiento, pues se tiene los modelos preentrenados para la aplicación. Pude demorar alrededor de 15 minutos cada entrenamiento.\n",
    "\n",
    "### **SVM kernel lineal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lBVQ-FIChRND"
   },
   "outputs": [],
   "source": [
    "def LinearUniqueTraining(X,y):\n",
    "    \n",
    "    from sklearn.svm import SVC\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "    \n",
    "    #entrenar el SVM con los parámetros optimos\n",
    "    \n",
    "    svl_model=SVC(kernel='linear',C=4).fit(X_train,y_train)\n",
    "    \n",
    "    svl_prediction=svl_model.predict(X_test)\n",
    "    \n",
    "    from sklearn.metrics import classification_report,confusion_matrix\n",
    "\t\n",
    "\t#Prec, Recall, F1\n",
    "    \n",
    "    print(classification_report(y_test,svl_prediction))\n",
    "\t\n",
    "\t#Confusión Matrix\n",
    "    \n",
    "    print(confusion_matrix(y_test,svl_prediction))\n",
    "    \n",
    "    #Guardar el modelo \n",
    "    \n",
    "    joblib.dump(svl_model, 'svl_model.pkl')\n",
    "\n",
    "LinearUniqueTraining(tm,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0k4wrkadoFt8"
   },
   "source": [
    "### **SVM kernel gaussiano**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0kAOhYCuhVsN"
   },
   "outputs": [],
   "source": [
    "def RbfUniqueTraining(X,y):\n",
    "    \n",
    "    from sklearn.svm import SVC\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "    \n",
    "    #entrenar el SVM con los parámetros optimos\n",
    "    \n",
    "    svg_model=SVC(kernel='rbf',C=2,gamma=4).fit(X_train,y_train)\n",
    "    \n",
    "    svg_prediction=svg_model.predict(X_test)\n",
    "    \n",
    "    from sklearn.metrics import classification_report,confusion_matrix\n",
    "    \n",
    "    print(classification_report(y_test,svg_prediction))\n",
    "    \n",
    "    \n",
    "    print(confusion_matrix(y_test,svg_prediction))\n",
    "    \n",
    "    #Guardar el modelo \n",
    "    \n",
    "    joblib.dump(svg_model, 'svg_model.pkl')\n",
    "\n",
    "RbfUniqueTraining(tm,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hU4LS8WjoFt-"
   },
   "source": [
    "### **Random Forest**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wTESvGFAhbKi"
   },
   "outputs": [],
   "source": [
    "def randomUniqueTraining(X,y):\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "    \n",
    "    #entrenar el RF con los parámetros optimos\n",
    "    \n",
    "    rf_model=RandomForestClassifier(n_estimators=742,min_samples_split=5,min_samples_leaf=1,max_features='auto',max_depth=110,bootstrap='false').fit(X_train,y_train)\n",
    "    \n",
    "    rf_prediction=rf_model.predict(X_test)\n",
    "    \n",
    "    from sklearn.metrics import classification_report,confusion_matrix\n",
    "\t\n",
    "    #Prec, Recall, F1\n",
    "\t\n",
    "    print(classification_report(y_test,rf_prediction))\n",
    "\t\n",
    "\t#Confusión Matrix\n",
    "    \n",
    "    print(confusion_matrix(y_test,rf_prediction))\n",
    "    \n",
    "    #Guardar el modelo \n",
    "    \n",
    "    joblib.dump(rf_model, 'rf_model.pkl')\n",
    "\n",
    "randomUniqueTraining(tm,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "08JuXP5GhgTv"
   },
   "source": [
    "#  **6. Discusión**\n",
    "\n",
    "Recordemos que el dataset original se dividio en 70% entrenamiento y el 30% en pruebas. A continuación se muestran las metricas de los modelos evaluados:\n",
    "\n",
    "![Imgur](https://i.imgur.com/ebNZyrd.jpg)\n",
    "\n",
    "## **Resultados sobre el dataset de Prueba**\n",
    "\n",
    "Se puede ver claramente que el accuracy de todos los modelos no fue prometedor después de evaluarse en el dataset de prueba. Cabe recordar que los datos no se encontraban balanceados a pesar de las técnicas empleadas. De manera que el accuracy no es la mejor alternativa para evaluar el modelo. Por lo que decidimos incluir además la precision, el recall y el f1 score, que son mejores indicadores en circunstancias como estas.\n",
    "\n",
    "## **Consideraciones sobre los resultados**\n",
    "\n",
    "* Como era de esperarse el Random Forest obtuvo un mejor desempeño que sus contrapartes. Esto puede explicarse con base en que el Random Forest Classifier esta diseñado para problemas de clasificación multiple y es menos sensible al datos desbalanceados.\n",
    "* Para empezar, recordemos que soló contabamos con 15000 datos de prueba, para entrenar un modelo de 1500 features. En este escenario vimos el pontencial de utilizar un SVM que están recomendados en estas situaciones. No obstante como es evidente el SVM tampoco funciono como esperabamos lo cual se le atribuye al desbalanceo de las clases.\n",
    "* El bajo desempeño relativo del SVM con kernel lineal, indica de que a pesar de que teniamos 1500 features para poder distinguir los ejemplos estos no eran suficientes para separar linealmente las clases. A pesar de esto, no era factible tampoco aumentar el número de features puesto que con un dataset tan pequeño nos arriesgabamos a causar overfitting.\n",
    "* Aunque el accuracy no impresiona en ninguno de los casos las otras medidas, como la precisión o el recall nos llevaron a seleccionar el algoritmo de Random Forest como el mejor modelo para resolver el problema. Teniendo en cuenta las limitaciones del proyecto como el tiempo, los recuersos computacionales y sobre todo los datos creemos que se cumplio la tarea.\n",
    "\n",
    "## **Estrategias de mejora**\n",
    "\n",
    "Datos, datos y más datos. Siendo un problema de NLP un modelo serio tendria que contar al menos con más de 7000 features por cada ejemplo de prueba puesto que las palabras del lenguaje humano tienen demasiadas variaciones que los modelos simplificados como este tienden a pasar por alto. De manera que una mejora significativa en el accuracy sólo podra conseguirse en la medida en que tengamos más datos, mejor distribuidos y de mejor calidad.\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F0VHnfI4h4JH"
   },
   "source": [
    " # **7. Aplicación**\n",
    "\n",
    "Con los modelos preentrenados, haremos una validación del comentario ingresado y mostraremos el resultado de este"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PxpJiv94v3dJ"
   },
   "outputs": [],
   "source": [
    "def dictionary_vector_r(review):\n",
    "    \n",
    "    words_dictionary = []\n",
    "    f = open(\"dictionary.txt\", \"r\")\n",
    "    for line in f:\n",
    "        words_dictionary.append(line.split(\"\\n\")[0])\n",
    "    f.close()\n",
    "    training_matrix = np.array([])\n",
    "    splitLine=review.split()\n",
    "    zeros = np.zeros(len(words_dictionary))\n",
    "    for word in splitLine:\n",
    "        if word in words_dictionary:\n",
    "            index = words_dictionary.index(word) \n",
    "            zeros[index]+=1\n",
    "    training_matrix = zeros\n",
    "    return training_matrix\n",
    "\n",
    "tm = dictionary_vector()\n",
    "tm = normalize(tm,axis=0)\n",
    "print(\"\\nHa cargado la matriz de datos con dimensiones de: \",np.shape(tm))\n",
    "\n",
    "def validation():\n",
    "  review = input(\"Ingrese su comentario: \")\n",
    "  review = normalizeReview(review)\n",
    "  review = dictionary_vector_r(review)\n",
    "\n",
    "  model_SVM_linear = joblib.load('svl_model.pkl')\n",
    "  print(\"SVM con Kernel lineal\\nCalificación predecida: \",model_SVM_linear.predict(review))\n",
    "  model_SVM_gauss = joblib.load('svg_model.pkl')\n",
    "  print(\"SVM con Kernel Gaussiano\\nCalificación predecida: \",model_SVM_gauss.predict(review))\n",
    "  model_RF = joblib.load('rf_model.pkl')\n",
    "  print(\"Random Forest \\nCalificación predecida: \",model_RF.predict(review))\n",
    "\n",
    "\n",
    "def menu():\n",
    "  s = \"\"\n",
    "  while(s!=\"2\"):\n",
    "  s = input(\"Bienvenido al clasificador de comentarios\\nSeleccione una opción\\n1. Escribe tu comentario\\n2. Salir\\n\")\n",
    "  if(s==\"1\"):\n",
    "    validation()\n",
    "  elif (s==\"2\"):\n",
    "    print(\"Gracias!\")\n",
    "  else:\n",
    "    print(\"Opcion Invalida\")\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Review_Proyect.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
